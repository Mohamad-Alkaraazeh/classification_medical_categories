{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":400,"status":"ok","timestamp":1663698080162,"user":{"displayName":"Osman Alb","userId":"12971803444625997364"},"user_tz":-120},"id":"Y48HukVGD54z","outputId":"78f504dd-c237-430d-a1df-194e2963a3e3"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1663698080550,"user":{"displayName":"Osman Alb","userId":"12971803444625997364"},"user_tz":-120},"id":"H8NB6bVLD548"},"outputs":[],"source":["import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18905,"status":"ok","timestamp":1663698099452,"user":{"displayName":"Osman Alb","userId":"12971803444625997364"},"user_tz":-120},"id":"KNgWM7TID55A","outputId":"81c0e482-53c8-49b4-a052-e74f202293d6"},"outputs":[],"source":["# load folder\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1663698099453,"user":{"displayName":"Osman Alb","userId":"12971803444625997364"},"user_tz":-120},"id":"WX5X9d_9D55F"},"outputs":[],"source":["# please change the path of your dataset here below\n","ROOT_PATH = \"/content/drive/MyDrive/colab notebook/data_exp/german_med_termss/\"\n","# ROOT_PATH= \"updated_german_med_terms/\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ol1_d3YGD55I","outputId":"ff0b305b-ec51-4e50-db03-d270527063fd"},"outputs":[],"source":["# # install all needed libraries\n","# !pip install transformers\n","# !pip install pytorch-lightning\n","# !pip install transformers datasets --quiet\n","# !pip install -U imbalanced-learn\n","# !pip install scikit-multilearn\n","\n","# !pip install spacy\n","# !pip install spacy-transformers\n","# !python3 -m spacy download de_dep_news_trf\n","# !python3 -m spacy download de_core_news_lg\n","\n","# # !python -m spacy download de_dep_news_trf\n","\n","!pip install transformers\n","!pip install pytorch-lightning\n","!pip install transformers datasets --quiet\n","!pip install -U imbalanced-learn\n","!pip install scikit-multilearn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_YXygSwCD55M"},"outputs":[],"source":["import torch\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","from transformers import BertTokenizerFast, BertForSequenceClassification, BertTokenizer, BertForMultipleChoice\n","from transformers import Trainer, TrainingArguments\n","import numpy as np\n","import random\n","from sklearn.model_selection import train_test_split\n","import os\n","import pandas as pd\n","import ast\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder\n","from imblearn.under_sampling import RandomUnderSampler\n","import datasets\n","from datasets import Dataset\n","\n","import torch\n","from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available\n","from transformers import BertTokenizerFast, BertForSequenceClassification, BertTokenizer\n","from transformers import Trainer, TrainingArguments\n","import numpy as np\n","import random\n","from sklearn.model_selection import train_test_split\n","import os\n","import pandas as pd\n","import ast\n","import seaborn as sns\n","from sklearn.preprocessing import LabelEncoder\n","from imblearn.under_sampling import RandomUnderSampler\n","import datasets\n","from datasets import Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OuQW3Y_VD55P"},"outputs":[],"source":["files = os.listdir(ROOT_PATH)\n","filepaths = [ROOT_PATH + f for f in os.listdir(ROOT_PATH) if f.endswith('.csv')]\n","df = pd.concat(map(pd.read_csv, filepaths))\n","df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOipfAZsD55T"},"outputs":[],"source":["def tx(x):\n","  res = []\n","  for v in ast.literal_eval(x):\n","    res.append(str(v).strip().lower())\n","  return res\n","\n","\n","df['labels_array']= df['expertise'].apply(lambda x: tx(x))\n","df = df[pd.notna(df['labels_array'])]\n","df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qAwlMBzcC2d"},"outputs":[],"source":["def get_categories_with_index(labels_classfified: dict):\n","    categories = []\n","    main = labels_classfified['Unnamed: 1']\n","    for k,v in main.items():\n","        if type(v) == str:\n","            categories.append(v)\n","    \n","    categories_with_index = []\n","    for i in range(len(categories) -1 ):\n","        start = list(main.values()).index(str(categories[i]))\n","        end = list(main.values()).index(str(categories[i + 1])) - 1\n","\n","        main_category = str(categories[i]).lower()\n","        categories_with_index.append({\"category\": categories[i], \"start\": start, \"end\": end, \"values\": [main_category]})\n","    \n","    # for the last category\n","    last_category = categories[-1].lower()\n","    categories_with_index.append({\"category\": last_category, \"start\": -1, \"end\": -1, \"values\": [last_category]})\n","    \n","    columns = list(labels_classfified.keys())[2:]\n","    for v in columns:\n","        for ci in categories_with_index:\n","            ci['category'] = str(ci['category']).lower()\n","            ci['values'] += found(labels_classfified[v], ci)\n","        \n","    return categories_with_index\n","\n","def found(dict_under, ci):\n","    values = []\n","    for k,v in dict_under.items():\n","        if k > ci['start'] and k < ci['end'] and type(v) == str:\n","            values += [str(v).lower()]\n","    \n","    return list(set(values))\n","\n","categories_with_index = get_categories_with_index(pd.read_excel('../content/drive/MyDrive/colab notebook/Praxisprojekt_MMC.xlsx').to_dict())\n","# categories_with_index = get_categories_with_index(pd.read_excel('Praxisprojekt_MMC.xlsx').to_dict())\n","target_names = [c['category'] for c in categories_with_index]\n","print(target_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAUNp_rHcC2f"},"outputs":[],"source":["def change_label_to_main_category(x):\n","    result = []\n","    for value_from_x in x:\n","        for ci in categories_with_index:\n","            if value_from_x in ci['values']:\n","                result.append(ci['category'])\n","\n","    return list(set(result))\n","\n","df['labels_array']= df['labels_array'].apply(lambda x: change_label_to_main_category(x))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OH7nkKXXcC2g"},"outputs":[],"source":["print(df['labels_array'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"29LAlRnXcC2h"},"outputs":[],"source":["\n","def get_number_of_sample_per_label(df, number_of_sample_per_label):\n","    result = []\n","\n","    for label, count in df['labels_array'].value_counts().items():\n","        if count <= number_of_sample_per_label:\n","            result += [label]\n","\n","    print(\"count -> \", len(result))\n","    return result\n","\n","found_labels = get_number_of_sample_per_label(df, 1)\n","print(found_labels)\n","\n","if len(found_labels) > 0:\n","    ri = []\n","    for index, row in df.iterrows():\n","        for l in found_labels:\n","            if l == row['labels_array']:\n","                ri.append(index)\n","\n","    print(\"ri\", ri)\n","    df.drop(index=ri, inplace = True)\n","    df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZcCBe7-icC2i"},"outputs":[],"source":["print(df['labels_array'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8CyuVR3cC2j"},"outputs":[],"source":["df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, stratify=df['labels_array'])\n","print(df_train.shape, df_val.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GF33Tg1vcC2j"},"outputs":[],"source":["def explode_doc(df):\n","    df['doc'] = df[['all_page', 'wiki_content']].apply(lambda x: [str(x['all_page']) ]+ [str(x['wiki_content']) ], axis=1)\n","    df = df.explode(\"doc\")\n","\n","    df = df[pd.notna(df['doc'])]\n","    df['doc'] = df['doc'].replace('nan', np.nan)\n","    df.dropna(subset=['doc'], inplace=True)  \n","\n","    return df\n","\n","def explode_labels_array(df):\n","\n","    df = df.explode(\"labels_array\")\n","    df.rename(columns = {'labels_array': 'label'}, inplace = True)\n","\n","    df['label'].replace('', np.nan, inplace=True)\n","    df.dropna(subset=['label'], inplace=True) \n","\n","    sns.set(rc={'figure.figsize':(11.7,8.27)})\n","    sns.countplot(data=df, y=\"label\", orient=\"v\")\n","\n","    return df\n","\n","def get_label_encoder(df):\n","    label_encoder = LabelEncoder()\n","    df['label'] = label_encoder.fit_transform(df['label'])\n","    # df['label'] = df['label'].apply(str)\n","\n","    le_name_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n","    print(le_name_mapping)\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2klMqZ78D55s"},"outputs":[],"source":["# print(\"---------------------------LABELS ARRAYS--------------------------------\")\n","# print(df_train['labels_array'].value_counts())\n","# print(\"-----------------------------------------------------------------\")\n","# print(df_val['labels_array'].value_counts())\n","\n","\n","df_train = get_label_encoder(explode_labels_array(explode_doc(df_train)))\n","df_val = get_label_encoder(explode_labels_array(explode_doc(df_val)))\n","\n","print(df_train.shape)\n","print(df_val.shape)\n","\n","# print(\"---------------------------LABELS--------------------------------\")\n","# print(df_train['label'].value_counts())\n","# print(\"-----------------------------------------------------------------\")\n","# print(df_val['label'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MMV3MQ7ScC2k"},"outputs":[],"source":["###################################################################\n","# df_train = df_train.head(10)\n","# df_val = df_val.head(2)\n","####################################################################"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ODL3YDlecC2l"},"outputs":[],"source":["models = [\n","    \"emilyalsentzer/Bio_ClinicalBERT\", # \n","    \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\", # \n","    \"bert-base-uncased\", # epoch = 2 --> F1 = 0.5346\n","    \"fspanda/Medical-Bio-BERT2\", #  \"----------------------------\"\n","    \"bert-base-multilingual-cased\", #\n","    \"bert-base-german-cased\", #\n","    \"smanjil/German-MedBERT\", #\n","    \"deutsche-telekom/bert-multi-english-german-squad2\", #\n","]\n","\n","\n","PRE_TRAINED_MODEL_NAME = models[5] # models[6] need 25 epochs for training\n","print(PRE_TRAINED_MODEL_NAME)\n","\n","MAX_LEN = 512\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zVr_4pEcC2m"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbS7-bWNcC2n"},"outputs":[],"source":["train_encodings = tokenizer(df_train['doc'].tolist(), truncation=True, padding=True, max_length=MAX_LEN)\n","valid_encodings = tokenizer(df_val['doc'].tolist(), truncation=True, padding=True, max_length=MAX_LEN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IYUr3nshcC2n"},"outputs":[],"source":["class ExpDataset(torch.utils.data.Dataset):\n","    def __init__(self, docs, labels):\n","        self.docs = docs\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {k: torch.tensor(v[idx]) for k, v in self.docs.items()}\n","        item[\"labels\"] = torch.tensor([self.labels.values[idx]])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wBlWg6bScC2n"},"outputs":[],"source":["train_dataset = ExpDataset(train_encodings, df_train['label'])\n","valid_dataset = ExpDataset(valid_encodings, df_val['label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTmjVhE0cC2o"},"outputs":[],"source":["NUM_LABELS = len(list(set(df_train['label'].tolist()))) # len(np.unique(df_train['label'])) neu hunzugefugt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x0xlB_H-cC2o"},"outputs":[],"source":["model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=NUM_LABELS).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFlOnUN_cC2p"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, classification_report, ConfusionMatrixDisplay\n","from sklearn.metrics import plot_confusion_matrix\n","\n","def compute_metrics(pred):\n","  labels = pred.label_ids\n","  preds = pred.predictions.argmax(-1)\n","\n","  print(\"classification_report --> HERE BELOW\")\n","  print(classification_report(labels, preds))\n","\n","  f1_score_micro = f1_score(labels, preds, average=\"micro\")\n","  accuracy = accuracy_score(labels, preds)\n","  recall = recall_score(labels, preds, average=\"micro\")\n","  precision = precision_score(labels, preds, average=\"micro\")\n","\n","  return {'F1_score' : f1_score_micro ,'Accuracy' : accuracy, 'Recall' :recall, 'Precision' : precision}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GNBvYHODcC2q"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir='./results',          # output directory\n","    num_train_epochs=10,# total number of training epochs\n","    learning_rate=2e-5,\n","    #fp16 = True,\n","    per_device_train_batch_size=8,  # batch size per device during training\n","    per_device_eval_batch_size=8,   # batch size for evaluation\n","    warmup_steps=120,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    save_total_limit = 5,\n","    logging_dir='./logs',            # directory for storing logs\n","    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n","    # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n","    # logging_steps=500,               # log & save weights each logging_steps\n","    save_strategy = \"epoch\",\n","    evaluation_strategy=\"epoch\",     # evaluate each `logging_steps`\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwL5BOmdcC2q"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,                         # the instantiated Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=valid_dataset,          # evaluation dataset\n","    compute_metrics=compute_metrics     # the callback that computes metrics of interest\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0WQ4PZ4ccC2q"},"outputs":[],"source":["# train the model\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJVl3HTgcC2r"},"outputs":[],"source":["# evaluate the current model after training\n","trainer.evaluate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iFc3Ak-kcC2r"},"outputs":[],"source":["predictions = trainer.predict(valid_dataset)\n","print(predictions.predictions.shape, predictions.label_ids.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OLNQQUIHcC2s"},"outputs":[],"source":["trues = [i[0] for i in predictions.label_ids]\n","preds = np.argmax(predictions.predictions, axis=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xniUquYIcC2s"},"outputs":[],"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","f,ax = plt.subplots(1,1,figsize=(40,40))\n","ConfusionMatrixDisplay.from_predictions(trues, preds, ax=ax)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtO2Q_16cC2t"},"outputs":[],"source":["# saving the fine tuned model & tokenizer\n","model_path = \"pubmed_\" + PRE_TRAINED_MODEL_NAME\n","model.save_pretrained(model_path)\n","tokenizer.save_pretrained(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3yFr__jCcC2u"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_NoQjQLBcC2u"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3.8.10 ('venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"0081987c862ade64fa8a6b449df670cc6c329610deb4a00a30d291630fdc88f2"}}},"nbformat":4,"nbformat_minor":0}
